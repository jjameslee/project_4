{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import the libraries needed throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We opened the pickled customized stopwords created from the previous notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./assets/stopwords.pkl','rb') as f:\n",
    "    stopwords = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is combined dataframe containing both the pre-disaster and post-disaster tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.read_csv('../project_4/assets/combined_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98550, 3)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we will implement the TFIDF tool to have the words recored as integer counts in order to be used on a the Logistic Regression model. We will also split our data into 'X' and 'y'. The X is an object that consists of the predictor column, in this case the text (tweets). The y will contain the binary classifier column, whether or not the tweets belongs to a disaster class or non-disaster class.\n",
    "We will randomly split the data into training and test sets. This is done so we can train our model on the training set and then evaluate the performance of the model on unseen new data (the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = combined_df['disaster']\n",
    "\n",
    "# Set X as text column.\n",
    "X = combined_df['text']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                   y, \n",
    "                                                    test_size=.30,\n",
    "                                                   random_state=42,\n",
    "                                                   stratify=y)\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words = stopwords, \n",
    "                        max_df=0.95, \n",
    "                        min_df=5, max_features=10_000)\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameslee/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "model = lr.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg Training score: 0.9591940276871783\n",
      "LogReg Testing score: 0.9512599357348216\n"
     ]
    }
   ],
   "source": [
    "print(f'LogReg Training score: {model.score(X_train_tfidf, y_train)}')\n",
    "print(f'LogReg Testing score: {model.score(X_test_tfidf, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA:\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is a form of feature extraction. We will apply LDA on a corpos of documents (tweets) and extract from it additive models of the topic structure of the corpus (collection of documents). The outcome will be a different topics and each of those topics are represented as a list of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_tfidf = pd.SparseDataFrame(X_train_tfidf,\n",
    "                             columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68915, 10000)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__</th>\n",
       "      <th>____</th>\n",
       "      <th>_hustle_junky</th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aan</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aaroncarter</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>...</th>\n",
       "      <th>zswaggers</th>\n",
       "      <th>zswagtour</th>\n",
       "      <th>zt</th>\n",
       "      <th>zu</th>\n",
       "      <th>zumba</th>\n",
       "      <th>zv</th>\n",
       "      <th>zw</th>\n",
       "      <th>zx</th>\n",
       "      <th>zy</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 10000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   __  ____  _hustle_junky  aa  aaa  aan  aaron  aaroncarter  ab  abandoned  \\\n",
       "0 NaN   NaN            NaN NaN  NaN  NaN    NaN          NaN NaN        NaN   \n",
       "1 NaN   NaN            NaN NaN  NaN  NaN    NaN          NaN NaN        NaN   \n",
       "2 NaN   NaN            NaN NaN  NaN  NaN    NaN          NaN NaN        NaN   \n",
       "3 NaN   NaN            NaN NaN  NaN  NaN    NaN          NaN NaN        NaN   \n",
       "4 NaN   NaN            NaN NaN  NaN  NaN    NaN          NaN NaN        NaN   \n",
       "\n",
       "   ...  zswaggers  zswagtour  zt  zu  zumba  zv  zw  zx  zy  zz  \n",
       "0  ...        NaN        NaN NaN NaN    NaN NaN NaN NaN NaN NaN  \n",
       "1  ...        NaN        NaN NaN NaN    NaN NaN NaN NaN NaN NaN  \n",
       "2  ...        NaN        NaN NaN NaN    NaN NaN NaN NaN NaN NaN  \n",
       "3  ...        NaN        NaN NaN NaN    NaN NaN NaN NaN NaN NaN  \n",
       "4  ...        NaN        NaN NaN NaN    NaN NaN NaN NaN NaN NaN  \n",
       "\n",
       "[5 rows x 10000 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df_tfidf.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df_tfidf = pd.SparseDataFrame(X_test_tfidf,\n",
    "                                    columns = tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29535, 10000)\n"
     ]
    }
   ],
   "source": [
    "X_test_df_tfidf.fillna(0, inplace=True)\n",
    "print(X_test_df_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA is a method of discovering topics from sentences. This is applicable to our project so we can better understand and identify the different topics of the tweets in our dataset. Due to our constraints of time and lack of in depth knowledge on LDA, we did not experiment with all the parameters of LDA. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: first tonight time south help look made way never big abc near pm guys pic would making high miles tomorrow\n",
      "Topic #1: homes residents home destroyed pic friends fema says return un power cnn hurricane photo show little thousand destruction next evacuate\n",
      "Topic #2: tx back going prayers still go update devastation traffic ever heart center may stop every water force families hate thoughts\n",
      "Topic #3: thousand mass people killed night one last dead least day borderline victims year pic say grill week sunday everything another\n",
      "Topic #4: new today watch latest ready state lol work pic begins check usa years beach taking read hurricane awesome crisis houstonstrong\n",
      "Topic #5: like know life got us god think real thanks fire long park already ahead pic things evacuations summer im looks\n",
      "Topic #6: get right repost safe want stay need see open let take days please someone even hope wait better team girl\n",
      "Topic #7: video great happy much thank mandatory morning evacuation pray world praying birthday area hours gonna amazing pic coming post love\n",
      "Topic #8: love good best really house city largo game getting hard family beautiful music said pic weekend looking early keep white\n",
      "Topic #9: hurricane pic west storm landfall category makes via live lower hit winds mph damage eye everyone come hits eyewall man\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=5,\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopwords)\n",
    "\n",
    "X_train_tf = tf_vectorizer.fit_transform(X_train)\n",
    "X_test_tf = tf_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                random_state=42)\n",
    "\n",
    "lda_train = lda.fit_transform(X_train_tf)\n",
    "lda_test = lda.transform(X_test_tf)\n",
    "\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this output, most of the topics are related to the hurricanes. Almost all of the 10 topics seem to be somehow related to the hurricane natural disaster, except topic 3. Topic 3 seems to relate more with the mass shooting that took place in the city of Thousand Oaks. \n",
    "\n",
    "Although LDA seems to be a popular method when it comes to textual analysis/language processing. LDA is primarily utilized to find latent (hidden) topics in the documents. Unfortunately it seems that LDA, where each word has a topic label, may not work well with Twitter as Twitter messages (tweets) are short and a single tweet is more likely to talk about one topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_loadings = pd.DataFrame(lda.components_, \n",
    "                                columns = tf_vectorizer.get_feature_names(),\n",
    "                                index = [f'topic_{x}' for x in range(lda.components_.shape[0])]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_loadings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a dataframe to map the terms of topic_0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tonight</th>\n",
       "      <td>385.136008</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100272</td>\n",
       "      <td>0.100018</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100013</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100017</td>\n",
       "      <td>0.100005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>354.815759</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.100019</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>0.100015</td>\n",
       "      <td>0.100017</td>\n",
       "      <td>0.100020</td>\n",
       "      <td>85.362585</td>\n",
       "      <td>0.100032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>south</th>\n",
       "      <td>353.936864</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>help</th>\n",
       "      <td>335.152664</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.100013</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.100020</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.100005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>315.891284</td>\n",
       "      <td>14.356540</td>\n",
       "      <td>58.066294</td>\n",
       "      <td>126.538391</td>\n",
       "      <td>0.100017</td>\n",
       "      <td>122.149780</td>\n",
       "      <td>0.100028</td>\n",
       "      <td>0.100032</td>\n",
       "      <td>0.100030</td>\n",
       "      <td>0.100010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>264.552605</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.100009</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100039</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.100011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>us</th>\n",
       "      <td>244.552141</td>\n",
       "      <td>0.100036</td>\n",
       "      <td>102.533737</td>\n",
       "      <td>0.100050</td>\n",
       "      <td>0.100022</td>\n",
       "      <td>69.504985</td>\n",
       "      <td>0.100024</td>\n",
       "      <td>58.825385</td>\n",
       "      <td>0.100027</td>\n",
       "      <td>56.552466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never</th>\n",
       "      <td>238.234392</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>0.100013</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100017</td>\n",
       "      <td>0.100013</td>\n",
       "      <td>0.100015</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.100003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pm</th>\n",
       "      <td>234.466910</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.100266</td>\n",
       "      <td>0.100018</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>0.100012</td>\n",
       "      <td>0.100015</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.100033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>devastation</th>\n",
       "      <td>227.402077</td>\n",
       "      <td>0.100029</td>\n",
       "      <td>40.700268</td>\n",
       "      <td>0.100005</td>\n",
       "      <td>0.100011</td>\n",
       "      <td>0.100007</td>\n",
       "      <td>0.100006</td>\n",
       "      <td>0.102899</td>\n",
       "      <td>0.100004</td>\n",
       "      <td>24.754948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                topic_0    topic_1     topic_2     topic_3   topic_4  \\\n",
       "tonight      385.136008   0.100005    0.100007    0.100272  0.100018   \n",
       "first        354.815759   0.100010    0.100012    0.100019  0.100011   \n",
       "south        353.936864   0.100011    0.100009    0.100008  0.100009   \n",
       "help         335.152664   0.100006    0.100013    0.100010  0.100008   \n",
       "one          315.891284  14.356540   58.066294  126.538391  0.100017   \n",
       "way          264.552605   0.100005    0.100010    0.100012  0.100009   \n",
       "us           244.552141   0.100036  102.533737    0.100050  0.100022   \n",
       "never        238.234392   0.100004    0.100011    0.100013  0.100010   \n",
       "pm           234.466910   0.100012    0.100266    0.100018  0.100012   \n",
       "devastation  227.402077   0.100029   40.700268    0.100005  0.100011   \n",
       "\n",
       "                topic_5   topic_6    topic_7    topic_8    topic_9  \n",
       "tonight        0.100008  0.100013   0.100010   0.100017   0.100005  \n",
       "first          0.100015  0.100017   0.100020  85.362585   0.100032  \n",
       "south          0.100006  0.100010   0.100008   0.100009   0.100026  \n",
       "help           0.100012  0.100020   0.100009   0.100012   0.100005  \n",
       "one          122.149780  0.100028   0.100032   0.100030   0.100010  \n",
       "way            0.100011  0.100010   0.100039   0.100012   0.100011  \n",
       "us            69.504985  0.100024  58.825385   0.100027  56.552466  \n",
       "never          0.100017  0.100013   0.100015   0.100010   0.100003  \n",
       "pm             0.100011  0.100012   0.100015   0.100006   0.100033  \n",
       "devastation    0.100007  0.100006   0.102899   0.100004  24.754948  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_loadings.sort_values('topic_0', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jameslee/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.8006.\n",
      "Testing Score: 0.8008.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate linear regression model.\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "\n",
    "\n",
    "# Fit on Z_train.\n",
    "logreg.fit(lda_train, y_train)\n",
    "\n",
    "# Score on training and testing sets.\n",
    "print(f'Training Score: {round(logreg.score(lda_train, y_train),4)}.')\n",
    "print(f'Testing Score: {round(logreg.score(lda_test, y_test),4)}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
